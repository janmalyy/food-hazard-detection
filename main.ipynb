{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdddcd5bddce9cb3",
   "metadata": {},
   "source": [
    "# PV056 project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9cd5d9",
   "metadata": {},
   "source": [
    "### Follow the instructions and run the cells in this notebook to reproduce all the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de6991",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T14:19:12.672929Z",
     "start_time": "2025-04-14T14:19:07.278417Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a23061",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T13:45:46.524355Z",
     "start_time": "2025-04-14T13:45:46.514359Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e0b4a6",
   "metadata": {},
   "source": [
    "## Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17db79d3f1e5b777",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T14:19:52.042546Z",
     "start_time": "2025-04-14T14:19:51.441139Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment if you want to download the dataset yourself\n",
    "dataset_parts = [\"train\", \"test\", \"valid\"]\n",
    "# for dataset_part in dataset_parts:\n",
    "#     url = f\"https://raw.githubusercontent.com/food-hazard-detection-semeval-2025/food-hazard-detection-semeval-2025.github.io/refs/heads/main/data/incidents_{dataset_part}.csv\"\n",
    "#     response = requests.get(url)\n",
    "#\n",
    "#     with open(f\"incidents_{dataset_part}.csv\", \"wb\") as f:\n",
    "#         f.write(response.content)\n",
    "\n",
    "trainset = pd.read_csv('incidents_train.csv', index_col=0)\n",
    "validset = pd.read_csv('incidents_valid.csv', index_col=0)\n",
    "testset = pd.read_csv('incidents_valid.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb71e6b5",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49708b466e042edd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T13:46:06.986919Z",
     "start_time": "2025-04-14T13:46:06.938893Z"
    }
   },
   "outputs": [],
   "source": [
    "trainset.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7007c0727298be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T13:46:08.718896Z",
     "start_time": "2025-04-14T13:46:08.548414Z"
    }
   },
   "outputs": [],
   "source": [
    "trainset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e8590b972c0541",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T13:46:14.363235Z",
     "start_time": "2025-04-14T13:46:14.347227Z"
    }
   },
   "outputs": [],
   "source": [
    "trainset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2800b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T13:46:34.035484Z",
     "start_time": "2025-04-14T13:46:34.026507Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    x = random.randint(0, len(trainset))\n",
    "    print(trainset[\"text\"][x])       # change the column name to view another column data\n",
    "    print()\n",
    "    print(\"XXX\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25de8f37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T13:49:02.056148Z",
     "start_time": "2025-04-14T13:49:01.879152Z"
    }
   },
   "outputs": [],
   "source": [
    "#DISTRIBUTION OF HAZARDS IN DATASET\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(trainset['hazard_category'].value_counts().index.to_list(), trainset['hazard_category'].value_counts().values, orientation='horizontal')\n",
    "\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Type of hazard')\n",
    "plt.title('Distribution of hazard category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b6d2bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T13:49:05.692102Z",
     "start_time": "2025-04-14T13:49:05.396243Z"
    }
   },
   "outputs": [],
   "source": [
    "#DISTRIBUTION OF PRODUCT TYPES IN DATASET\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(trainset['product_category'].value_counts().index.to_list(), trainset['product_category'].value_counts().values, orientation='horizontal')\n",
    "\n",
    "plt.xlabel('Type of product')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of product category')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cf6e5a",
   "metadata": {},
   "source": [
    "## Generate synthetic data for rare product and hazard categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f8aabd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T13:49:22.070859Z",
     "start_time": "2025-04-14T13:49:22.064858Z"
    }
   },
   "outputs": [],
   "source": [
    "from food_hazard_detection.balance_dataset import (generate_prompt_triplets_by_hazard, generate_prompt_triplets_by_product,\n",
    "                             generate_synthetic_data)\n",
    "\n",
    "from food_hazard_detection.settings import FILES_DIR, SYNTHETIC_DATA_DIR\n",
    "\n",
    "rare_hazard_categories = [\"migration\", \"food additives and flavourings\",\n",
    "                              \"organoleptic aspects\", \"packaging defect\"]\n",
    "rare_product_categories = [\"sugars and syrups\", \"feed materials\", \"food contact materials\",\n",
    "                           \"honey and royal jelly\", \"food additives and flavourings\", \"fats and oils\",\n",
    "                           \"pet feed\", \"other food product / mixed\", \"alcoholic beverages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c124c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T13:50:22.658098Z",
     "start_time": "2025-04-14T13:50:21.908752Z"
    }
   },
   "outputs": [],
   "source": [
    "combinations_hazard = generate_prompt_triplets_by_hazard(rare_hazard_categories, trainset)\n",
    "# Uncomment if you want really to generate the data. It takes some time.\n",
    "# generate_synthetic_data(SYNTHETIC_DATA_DIR / \"synthetic_data_hazard.csv\",\n",
    "#                         FILES_DIR / \"prompts/generate_synthetic_data.md\", combinations_hazard)\n",
    "\n",
    "combinations_product = generate_prompt_triplets_by_product(rare_product_categories, trainset)\n",
    "# generate_synthetic_data(SYNTHETIC_DATA_DIR / \"synthetic_data_product.csv\",\n",
    "#                         FILES_DIR / \"prompts/generate_synthetic_data.md\", combinations_product)\n",
    "\n",
    "print(\"Number of to-be generated synthetic data points:\", len(combinations_hazard)+len(combinations_product))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2d7e6a",
   "metadata": {},
   "source": [
    "The data produced by Mistral are not perfect. So at this point some manual curration is needed (e.g. quote the text column to parse the csv properly or drop some rows with missing values). Because of that, we use later in the code already preprocessed synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca90c09e",
   "metadata": {},
   "source": [
    "Load and check the generated data.\n",
    "We generated smaller and bigger amount of synthetic data and we want to test whether it has some impact on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300fd279",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T14:15:26.091375Z",
     "start_time": "2025-04-14T14:15:26.050376Z"
    }
   },
   "outputs": [],
   "source": [
    "synthetic_data_small = pd.read_csv(SYNTHETIC_DATA_DIR / \"synthetic_data_small.csv\")\n",
    "print(synthetic_data_small.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2948c9934029bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T14:15:20.470443Z",
     "start_time": "2025-04-14T14:15:20.424244Z"
    }
   },
   "outputs": [],
   "source": [
    "synthetic_data_big = pd.read_csv(SYNTHETIC_DATA_DIR / \"synthetic_data_big.csv\")\n",
    "print(synthetic_data_big.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf83f2766454b0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T14:17:53.312240Z",
     "start_time": "2025-04-14T14:17:53.264874Z"
    }
   },
   "outputs": [],
   "source": [
    "train_with_small = pd.concat([trainset, synthetic_data_small])\n",
    "train_with_big = pd.concat([trainset, synthetic_data_big])\n",
    "train_with_big.info()\n",
    "train_with_small.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23650d4bfd637958",
   "metadata": {},
   "source": [
    "### Preprocess the data\n",
    "\n",
    "\n",
    "1. As we intend to use, among ther models, a TF-IDF-based model, it is necessary to remove stop words and punctuation, then apply tokenization and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c9fc8d2876dcd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T14:22:53.746549Z",
     "start_time": "2025-04-14T14:22:53.738261Z"
    }
   },
   "outputs": [],
   "source": [
    "from food_hazard_detection.preprocessing import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9cd93266806d3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T14:22:56.501937Z",
     "start_time": "2025-04-14T14:22:55.407161Z"
    }
   },
   "outputs": [],
   "source": [
    "trainset['text_preprocessed'] = trainset.text.apply(lambda x: preprocessing(x))\n",
    "validset['text_preprocessed'] = validset.text.apply(lambda x: preprocessing(x))\n",
    "testset['text_preprocessed'] = testset.text.apply(lambda x: preprocessing(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25967844db22e0c",
   "metadata": {},
   "source": [
    "### Load high-level features from text with LLM (gpt-4o-mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec701eaf6d603c99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T14:21:19.102917Z",
     "start_time": "2025-04-14T14:21:19.092404Z"
    }
   },
   "outputs": [],
   "source": [
    "from food_hazard_detection.preprocessing import process_txt_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1391053b15425c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T14:23:32.542066Z",
     "start_time": "2025-04-14T14:23:31.592811Z"
    }
   },
   "outputs": [],
   "source": [
    "folder_path = FILES_DIR / \"datasets/llm_features/outputs\"\n",
    "df_llm_feats = process_txt_files(folder_path, \"hazard\")\n",
    "df_llm_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f6b1050066182b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T14:23:43.802321Z",
     "start_time": "2025-04-14T14:23:42.733202Z"
    }
   },
   "outputs": [],
   "source": [
    "df_llm_feats = df_llm_feats.drop(columns=[\"id\", \"custom_id\", \"recall_date\", \"company_name\", \"product_batch_code\", 'product_size'], errors='ignore')\n",
    "\n",
    "\n",
    "for col in df_llm_feats.columns:\n",
    "    df_llm_feats[col] = df_llm_feats[col].apply(lambda x: str(x) if isinstance(x, list) else x)\n",
    "\n",
    "df_llm_feats = pd.get_dummies(df_llm_feats, sparse=False, prefix_sep='_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56b12d78a1dd1f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T14:23:46.602339Z",
     "start_time": "2025-04-14T14:23:46.584955Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_score(hazards_true, products_true, hazards_pred, products_pred):\n",
    "    # compute f1 for hazards:\n",
    "    f1_hazards = f1_score(\n",
    "        hazards_true,\n",
    "        hazards_pred,\n",
    "        average='macro'\n",
    "    )\n",
    "    print(f\"F1 for hazard_category: {round(f1_hazards, 2)}\")\n",
    "    # compute f1 for products:\n",
    "    f1_products = f1_score(\n",
    "        products_true[hazards_pred == hazards_true],\n",
    "        products_pred[hazards_pred == hazards_true],\n",
    "        average='macro'\n",
    "    )\n",
    "    print(f\"F1 for product_category: {round(f1_products, 2)}\")\n",
    "    return (f1_hazards + f1_products) / 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac90804ccc86b6b3",
   "metadata": {},
   "source": [
    "# Sub-Task 1 - LLM features only\n",
    "This task consists of predicting 2 category labels:\n",
    "- hazard_category: the type of hazard (e.g. microbiological, chemical, etc.)\n",
    "- product_category: the type of product (e.g. meat, fish, etc.)\n",
    "\n",
    "Observed metric: weighted F1 score - hazard_category is preffered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9110e8070b0fe0",
   "metadata": {},
   "source": [
    "### hazard_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3026b71e1615be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T14:25:04.364099Z",
     "start_time": "2025-04-14T14:25:04.352221Z"
    }
   },
   "outputs": [],
   "source": [
    "#features\n",
    "X_train = df_llm_feats.iloc[:trainset.shape[0]]\n",
    "X_val = df_llm_feats.iloc[trainset.shape[0]:trainset.shape[0] + validset.shape[0]]\n",
    "X_test = df_llm_feats.tail(testset.shape[0])\n",
    "#labels\n",
    "y_train = trainset['hazard_category']\n",
    "y_val = validset['hazard_category']\n",
    "y_test = testset['hazard_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdeed0b4d73fcdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T14:26:05.323688Z",
     "start_time": "2025-04-14T14:25:07.614824Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_dist = {\n",
    "    \"n_estimators\": [50, 100, 200, 400],       # Počet stromů v lese\n",
    "    \"max_depth\": [3, 5, 10, None],        # Maximální hloubka stromu\n",
    "    \"min_samples_split\": [2, 5, 10],      # Minimální počet vzorků pro split\n",
    "    \"min_samples_leaf\": [1, 2, 5, 10],        # Minimální počet vzorků v listu\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=10,\n",
    "    scoring=\"f1_macro\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best paramaters: \", random_search.best_params_)\n",
    "print(\"Best score on training CV: \", random_search.best_score_)\n",
    "hazard_true = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98641b8f7946b80c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T11:17:10.618608Z",
     "start_time": "2025-04-14T11:17:10.552959Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model = random_search.best_estimator_\n",
    "hazard_pred = best_model.predict(X_val)\n",
    "print(\"Classification report na testu:\")\n",
    "print(classification_report(hazard_true ,hazard_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a8c6bc9f9833a2",
   "metadata": {},
   "source": [
    "### product_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43930b823825ab58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T11:17:10.684887Z",
     "start_time": "2025-04-14T11:17:10.678685Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = trainset['product_category']\n",
    "y_val = validset['product_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e317ff806c6d64a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T11:17:25.909564Z",
     "start_time": "2025-04-14T11:17:10.748887Z"
    }
   },
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(\n",
    "    model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=10,\n",
    "    scoring=\"f1_macro\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best paramaters: \", random_search.best_params_)\n",
    "print(\"Best score on training CV: \", random_search.best_score_)\n",
    "product_true = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92da38615dc27f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T11:17:26.080700Z",
     "start_time": "2025-04-14T11:17:26.026602Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model = random_search.best_estimator_\n",
    "product_pred = best_model.predict(X_val)\n",
    "print(\"Classification report na testu:\")\n",
    "print(classification_report(product_true, product_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e715e0082e673e",
   "metadata": {},
   "source": [
    "### Sub-Task 1 results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefc31af0c0f02f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T11:17:59.044992Z",
     "start_time": "2025-04-14T11:17:59.016284Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Final ST1 F1 score: {round(compute_score(hazard_true, product_true, hazard_pred, product_pred), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fe2ea2ce3cf071",
   "metadata": {},
   "source": [
    "# Sub-Task 2 - LLM features only\n",
    "This task consists of predicting 2 concrete labels:\n",
    "- hazard - the type of hazard (e.g. salmonella, etc.)\n",
    "- product -  the type of product (e.g. chicken, etc.)\n",
    "\n",
    "Observed metric: weighted F1 score - hazard is preffered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29600f804088c43a",
   "metadata": {},
   "source": [
    "### hazard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff1cff728a3ea0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T11:20:25.226802Z",
     "start_time": "2025-04-14T11:20:05.593772Z"
    }
   },
   "outputs": [],
   "source": [
    "#features\n",
    "X_train = df_llm_feats.iloc[:trainset.shape[0]]\n",
    "X_val = df_llm_feats.iloc[trainset.shape[0]:trainset.shape[0] + validset.shape[0]]\n",
    "X_test = df_llm_feats.tail(testset.shape[0])\n",
    "#labels\n",
    "y_train = trainset['hazard']\n",
    "y_val = validset['hazard']\n",
    "y_test = testset['hazard']\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=10,\n",
    "    scoring=\"f1_macro\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best paramaters: \", random_search.best_params_)\n",
    "print(\"Best score on training CV: \", random_search.best_score_)\n",
    "hazard_true = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdae1469c523d9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T11:21:09.004777Z",
     "start_time": "2025-04-14T11:21:08.906012Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model = random_search.best_estimator_\n",
    "hazard_pred = best_model.predict(X_val)\n",
    "print(\"Classification report na testu:\")\n",
    "print(classification_report(hazard_true ,hazard_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b1bf8f0e4c18da",
   "metadata": {},
   "source": [
    "### product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adbc7b8de08f119",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T11:24:36.740981Z",
     "start_time": "2025-04-14T11:21:11.709867Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = trainset['product']\n",
    "y_val = validset['product']\n",
    "random_search = RandomizedSearchCV(\n",
    "    model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=10,\n",
    "    scoring=\"f1_macro\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best paramaters: \", random_search.best_params_)\n",
    "print(\"Best score on training CV: \", random_search.best_score_)\n",
    "product_true = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cddee199eeb4c7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T14:10:20.574233Z",
     "start_time": "2025-04-14T14:10:20.428871Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model = random_search.best_estimator_\n",
    "product_pred = best_model.predict(X_val)\n",
    "print(\"Classification report na testu:\")\n",
    "print(classification_report(product_true, product_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29935125ca5d8f6",
   "metadata": {},
   "source": [
    "### Sub-Task 2 results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38785b97b37e3f97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T11:24:38.312426Z",
     "start_time": "2025-04-14T11:24:38.276881Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Final ST2 F1 score: {round(compute_score(hazard_true, product_true, hazard_pred, product_pred), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5746768",
   "metadata": {},
   "source": [
    "### BERT and RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca41df8e-cea2-458c-8e9b-63c75259883c",
   "metadata": {},
   "source": [
    "In this part, we use two BERT based models - distillbert and roberta - to predict categories. We use \"title\" and \"text\" columns together as an input. Since BERT can only predict single cathegory, we must train each model 2x times.\n",
    "\n",
    "Warning: the models takee a very long timeto fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9c414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, BertTokenizer, BertForSequenceClassification, DataCollatorWithPadding\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751317b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_labels(dataset, column, label_dict):\n",
    "    # autoencoding\n",
    "    unique_values = dataset[column].unique()\n",
    "    for value in unique_values:\n",
    "        if value not in label_dict:\n",
    "            label_dict[value] = len(label_dict)\n",
    "\n",
    "\n",
    "def transform_dataset(dataset):\n",
    "    for split in dataset.keys():\n",
    "        dataset[split] = dataset[split].replace({\n",
    "            \"hazard_category\": labels_hazard_categories,\n",
    "            \"product_category\": labels_product_categories,\n",
    "            \"product\": labels_products,\n",
    "            \"hazard\": labels_hazards\n",
    "        })\n",
    "        dataset[split][\"text\"] = dataset[split][\"title\"] + \": \" + dataset[split][\"text\"]\n",
    "        dataset[split] = dataset[split].drop(columns=[\"day\", \"month\", \"year\", \"country\", \"title\"])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def compute_score(hazards_true, products_true, hazards_pred, products_pred):\n",
    "    # compute f1 for hazards:\n",
    "    f1_hazards = f1_score(\n",
    "        hazards_true,\n",
    "        hazards_pred,\n",
    "        average='macro'\n",
    "    )\n",
    "    print(\"Hazard score\", f1_hazards)\n",
    "    # compute f1 for products:\n",
    "    f1_products = f1_score(\n",
    "        products_true,\n",
    "        products_pred,\n",
    "        average='macro'\n",
    "    )\n",
    "    print(\"Product score\", f1_products)\n",
    "    return (f1_hazards + f1_products) / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5768ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_hazard_categories = {}\n",
    "labels_hazards = {}\n",
    "labels_product_categories = {}\n",
    "labels_products = {}\n",
    "\n",
    "unique_products = set()\n",
    "for split in [train_with_big, validset, testset]:\n",
    "    unique_products.update(split[\"product\"])\n",
    "\n",
    "assign_labels(train_with_big, \"hazard_category\", labels_hazard_categories)\n",
    "assign_labels(train_with_big, \"hazard\", labels_hazards)\n",
    "assign_labels(train_with_big, \"product_category\", labels_product_categories)\n",
    "\n",
    "for value in unique_products:\n",
    "    if value not in labels_products:\n",
    "        labels_products[value] = len(labels_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb8ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_dataset = transform_dataset({\"train\": train_with_big.copy(), \"valid\": validset.copy(), \"test\": testset.copy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f18cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_dataset = DatasetDict({\"train\" : Dataset.from_pandas(bert_dataset[\"train\"]),\"valid\" : Dataset.from_pandas(bert_dataset[\"valid\"]), \"test\" : Dataset.from_pandas(bert_dataset[\"test\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c8eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "def bert_tokenize_function(examples):\n",
    "    return bert_tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70097ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenized = bert_dataset.map(bert_tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5531a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_data_collator = DataCollatorWithPadding(tokenizer=bert_tokenizer)\n",
    "\n",
    "bert_training_args = TrainingArguments(\"test_trainer\",\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18b8df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_trainers = dict()\n",
    "\n",
    "length = {'hazard' : len(labels_hazards), 'product' : len(labels_products), 'hazard_category' : len(labels_hazard_categories), 'product_category' : len(labels_product_categories)} \n",
    "\n",
    "for label in ['product', 'hazard', 'product_category', 'hazard_category']:\n",
    "    train_dataset = bert_tokenized[\"train\"].rename_column(label, \"label\")\n",
    "    eval_dataset = bert_tokenized[\"valid\"].rename_column(label, \"label\")\n",
    "\n",
    "    bert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert/distilbert-base-uncased\", num_labels=length[label]\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=bert_model,\n",
    "        args=bert_training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        processing_class=bert_tokenizer,\n",
    "        data_collator=bert_data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    bert_trainers[label] = trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5333df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset_bert = dict()\n",
    "\n",
    "for category in bert_trainers.keys():\n",
    "    devset_bert[category] = bert_trainers[category].predict(bert_tokenized['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91090846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "print(f\"Score Sub-Task 1: {compute_score(bert_tokenized['test']['hazard_category'], bert_tokenized['test']['product_category'], devset_bert['hazard_category'].predictions.argmax(-1).tolist(), devset_bert['product_category'].predictions.argmax(-1).tolist()):.3f}\")\n",
    "print(f\"Score Sub-Task 2: {compute_score(bert_tokenized['test']['hazard'], bert_tokenized['test']['product'], devset_bert['hazard'].predictions.argmax(-1), devset_bert['product'].predictions.argmax(-1)):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf7e0ce-1121-4d07-bd6f-f1081f33287d",
   "metadata": {},
   "source": [
    "F1 score after training was:\n",
    "\n",
    "Hazard score 0.8453172117313383\n",
    "Product score 0.646217380479502\n",
    "Hazard Category score 0.45974956313754045\n",
    "Product Category score 0.11395288958485314\n",
    "\n",
    "DistilBERT is thus better on predicting than baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eeaecc",
   "metadata": {},
   "source": [
    "### Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41a1877",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_dataset = transform_dataset({\"train\": trainset.copy(), \"valid\": validset.copy(), \"test\": testset.copy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d61eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_dataset = DatasetDict({\"train\" : Dataset.from_pandas(roberta_dataset[\"train\"]),\"valid\" : Dataset.from_pandas(roberta_dataset[\"valid\"]), \"test\" : Dataset.from_pandas(roberta_dataset[\"test\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692731b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def roberta_tokenize_function(examples):\n",
    "    return roberta_tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba136cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenized = roberta_dataset.map(roberta_tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fea9c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_data_collator = DataCollatorWithPadding(tokenizer=roberta_tokenizer)\n",
    "\n",
    "roberta_training_args = TrainingArguments(\"test_trainer\",\n",
    "                                  num_train_epochs=3,\n",
    "                                  weight_decay=0.01,\n",
    "                                  eval_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50b07bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_trainers = dict()\n",
    "\n",
    "for label in ['product', 'hazard', 'product_category', 'hazard_category']:\n",
    "    train_dataset = roberta_tokenized[\"train\"].rename_column(label, \"label\")\n",
    "    eval_dataset = roberta_tokenized[\"valid\"].rename_column(label, \"label\")\n",
    "\n",
    "    roberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=length[label])\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=roberta_model,\n",
    "        args=roberta_training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        processing_class=roberta_tokenizer,\n",
    "        data_collator=roberta_data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    bert_trainers[label] = trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c68ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset_roberta = dict()\n",
    "\n",
    "for category in bert_trainers.keys():\n",
    "    devset_roberta[category] = roberta_trainers[category].predict(roberta_tokenized['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149446c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Score Sub-Task 1: {compute_score(bert_tokenized['test']['hazard_category'], bert_tokenized['test']['product_category'], devset_bert['hazard_category'].predictions.argmax(-1).tolist(), devset_bert['product_category'].predictions.argmax(-1).tolist()):.3f}\")\n",
    "\n",
    "print(f\"Score Sub-Task 2: {compute_score(bert_tokenized['test']['hazard'], bert_tokenized['test']['product'], devset_bert['hazard'].predictions.argmax(-1), devset_bert['product'].predictions.argmax(-1)):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
